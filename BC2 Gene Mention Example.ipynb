{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioCreative II Gene Mention (GM) Task\n",
    "\n",
    "For more information: https://biocreative.bioinformatics.udel.edu/tasks/biocreative-ii/task-1a-gene-mention-tagging/\n",
    "\n",
    "## Training Data\n",
    "\n",
    "The training data is described in the corpus README.GM file, but I'll describe it here as well. Training data consists of a sentences file `train.in` and a label file `GENE.eval`. The eval file lists the offsets of any gene mentions (there may be none for any sentence). It is easiest to understand using an example using the first sentence from `train.in`:\n",
    "\n",
    "```\n",
    "P00001606T0076 Comparison with alkaline phosphatases and 5-nucleotidase\n",
    "```\n",
    "Each line contains a single sentence, starting with a unique sentence identifier, followed by the text. This particular sample contains two (2) gene mentions, listed on two lines in the `GENE.eval` file:\n",
    "\n",
    "```\n",
    "P00001606T0076|14 33|alkaline phosphatases\n",
    "P00001606T0076|37 50|5-nucleotidase\n",
    "```\n",
    "The first field (delimited by the bar symbols) is the matching sentence ID. The second field contains the offset of the first and last characters in the GM, _not counting space characters_. So, looking at _alkaline phosphatases_, the first letter `a` is at offset 14 keeping in mind that the first character in the sentence is offset 0. If you are not careful, you may think the offset of `a` is 16, but remember that spaces are not counted. Counting in a similar way, the last `s` in _phosphatases_ is at offset 33.\n",
    "\n",
    "## Prepare the training data\n",
    "\n",
    "This format is not very convenient for training our ML model. One method used to train NER systems is to label each sentence token with either 'B','I', or 'O' where 'B' marks the beginning token in an entity, 'I' marks subsequent tokens in a multi-token entity (*inside*), and 'O' is for tokens *outside* the entity. For this example, we will use a Keras model to perform _sequence tagging_, where the tags are 'B', 'I', and 'O'. This is very similar to how one would perform full NER or POS tagging.\n",
    "\n",
    "The module *bc2reader.py* is provided to help convert these two files to something more usable. The first argument to the `BC2Reader` contructor is the sentence file. The second is the gene mention file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bc2reader import BC2Reader\n",
    "import os\n",
    "\n",
    "train_home = '/home/ryan/Development/deep-learn-bio-nlp/bc2/bc2geneMention/train'\n",
    "reader = BC2Reader('{0}/train.in'.format(train_home), '{0}/GENE.eval'.format(train_home))\n",
    "reader.convert('{0}/train.json'.format(train_home))\n",
    "# vocab = [a for a, b in reader.vocab.items() if b >= 3] # Let's see the word at least 3 times\n",
    "vocab = [a for a, _ in reader.vocab.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 36655\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of vocabulary: {0}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a JSON file with a more familiar format. Here is the first sentence in our BIO format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P00001606T0076', ['Comparison', 'with', 'alkaline', 'phosphatases', 'and', '5-nucleotidase'], ['O', 'O', 'B', 'I', 'O', 'B']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('{0}/train.json'.format(train_home), 'r') as json_file:\n",
    "    training_data = json.load(json_file)\n",
    "    print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be easier to read if we zip together the tokens and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Comparison', 'O'), ('with', 'O'), ('alkaline', 'B'), ('phosphatases', 'I'), ('and', 'O'), ('5-nucleotidase', 'B')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(training_data[0][1], training_data[0][2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the test data\n",
    "\n",
    "The same _bc2reader_ is used to format the test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BC2GM000008491', ['Phenotypic', 'analysis', 'demonstrates', 'that', 'trio', 'and', 'Abl', 'cooperate', 'in', 'regulating', 'axon', 'outgrowth', 'in', 'the', 'embryonic', 'central', 'nervous', 'system', '(', 'CNS', ')', '.'], ['O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "test_home = '/home/ryan/Development/deep-learn-bio-nlp/bc2/bc2GNandGMgold_Subs/sourceforgeDistrib-22-Sept-07/genemention/BC2GM/test'\n",
    "reader = BC2Reader('{0}/test.in'.format(test_home), '{0}/GENE.eval'.format(test_home))\n",
    "reader.convert('{0}/test.json'.format(test_home))\n",
    "\n",
    "with open('{0}/test.json'.format(test_home), 'r') as json_file:\n",
    "    test_data = json.load(json_file)\n",
    "    print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 90.33062421615003\n",
      "1: 4.387337922044577\n",
      "2: 5.282037861805398\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAG9VJREFUeJzt3W1sVGXex/HvTFuw3bFlHkq7xZJsoU0koduS6RqrUFxGYtQYbiREjBrwAcloSNqoCxvjvkBwDNYiSRsTNWTRNxBD6yb3C5Kh0iZ2EwbaorG7QkESTUuHzhkLRVgoPfcL49yytMxDpw/0/D6vOlevc+Y6/wz8el1zHmymaZqIiIgl2ad7ACIiMn0UAiIiFqYQEBGxMIWAiIiFKQRERCxMISAiYmEKARERC1MIiIhYmEJARMTCMuN1GBwcpLGxkZ9++gmbzYbP5+PRRx9leHiYhoYGLly4QH5+PrW1tTgcDkzTZN++fXR1dTF37lz8fj8lJSUAHD16lEOHDgGwdu1aVq5cOakHJyIitxc3BDIyMnj22WcpKSnhypUrbNu2jfLyco4ePcrSpUtZs2YNLS0ttLS08Mwzz9DV1cX58+fZu3cvp0+f5uOPP2bXrl0MDw/z+eefEwgEANi2bRterxeHwxF3kH19fUkdlMfjYXBwMKltrEh1ik81SozqFN9U1qioqCjhvnGXg5xOZ+wv+ezsbBYsWIBhGIRCIWpqagCoqakhFAoBcPz4cVasWIHNZqOsrIzLly8TjUbp7u6mvLwch8OBw+GgvLyc7u7uVI5PRETSJO5M4LfC4TDff/89ixcvZmhoCKfTCfwSFBcvXgTAMAw8Hk9sG7fbjWEYGIaB2+2OtbtcLgzDGPN9gsEgwWAQgEAgcNP+EjqozMykt7Ei1Sk+1SgxqlN8M7VGCYfA1atXqa+vZ+PGjeTk5Izbb6ybktpstjH7jtfu8/nw+Xyx18lOoTQ1TYzqFJ9qlBjVKb47djkIYGRkhPr6epYvX859990HQF5eHtFoFIBoNEpubi7wy1/+vz3QSCSC0+nE5XIRiURi7YZhxGYSIiIyPeKGgGmafPjhhyxYsIDHH3881u71emlrawOgra2NqqqqWHt7ezumaXLq1ClycnJwOp1UVFRw8uRJhoeHGR4e5uTJk1RUVEzSYYmISCLiLgd99913tLe3s3DhQl5//XUANmzYwJo1a2hoaKC1tRWPx0NdXR0AlZWVdHZ2snXrVubMmYPf7wfA4XDw5JNPsn37dgDWrVuX0JlBIiIyeWx3wpPFdIro5FCd4lONEqM6xXdHfycgIiKzk0JARMTCkrpOQBJz46UnxmzP+OgfUzwSEZHb00xARMTCFAIiIhamEBARsTB9JzAB4639i4jcKTQTEBGxMIWAiIiFKQRERCxMISAiYmEKARERC1MIiIhYmEJARMTCFAIiIhamEBARsTCFgIiIhcW9bURTUxOdnZ3k5eVRX18PQENDQ+xpXz///DM5OTns3r2bcDhMbW1t7Kk2paWlbN68GYCzZ8/S2NjItWvXqKysZNOmTdhstsk6LhERSUDcEFi5ciWPPPIIjY2Nsbba2trYz/v37ycnJyf2urCwkN27d9+yn48++oiXX36Z0tJS3nnnHbq7u6msrJzo+EVEZALiLgctWbJk3AfCm6bJP//5Tx544IHb7iMajXLlyhXKysqw2WysWLGCUCiU2ohFRCRtJnQX0X/961/k5eXx+9//PtYWDod54403yM7O5qmnnuLee+/FMAzcbnesj9vtxjCMiby1iIikwYRC4KuvvrppFuB0OmlqauLuu+/m7Nmz7N69m/r6ekzTTGq/wWCQYDAIQCAQwOPxJLV9ZmZm0tukYiDJ/lMxpmRMVZ3uZKpRYlSn+GZqjVIOgRs3bnDs2DECgUCsLSsri6ysLABKSkooKCigv78ft9tNJBKJ9YtEIrhcrnH37fP58Pl8sdeDg4NJjc3j8SS9zVQY+J/qpPpP9jOJZ2qdZhLVKDGqU3xTWaNfT85JRMqniH7zzTcUFRXdtMxz8eJFRkdHARgYGKC/v5+CggKcTifZ2dmcOnUK0zRpb2/H6/Wm+tYiIpImcWcCe/bsoaenh0uXLrFlyxbWr1/Pn//851uWggB6eno4ePAgGRkZ2O12XnrppdiXyi+++CJNTU1cu3aNiooKnRkkIjID2MxkF+ynwa/XJCRqqqZdk/14SS0HTT/VKDGqU3yzbjlIRETufAoBERELm9ApolYx2cs+IiLTRTMBERELUwiIiFiYQkBExMIUAiIiFqYQEBGxMIWAiIiFKQRERCxMISAiYmEKARERC1MIiIhYmEJARMTCFAIiIhamEBARsTCFgIiIhSkEREQsLO7zBJqamujs7CQvL4/6+noADh48yJEjR8jNzQVgw4YNLFu2DIDm5mZaW1ux2+1s2rSJiooKALq7u9m3bx+jo6OsWrWKNWvWTNYxiYhIguKGwMqVK3nkkUdobGy8qf2xxx7jiSduftjKjz/+SEdHB++//z7RaJQdO3bwwQcfAPDJJ5/w5ptv4na72b59O16vl3vuuSeNhyIiIsmKGwJLliwhHA4ntLNQKER1dTVZWVnMnz+fwsJCent7ASgsLKSgoACA6upqQqGQQkBEZJql/HjJw4cP097eTklJCc899xwOhwPDMCgtLY31cblcGIYBgNvtjrW73W5Onz49gWGLiEg6pBQCq1evZt26dQAcOHCA/fv34/f7MU1zzP5jtdtstnH3HwwGCQaDAAQCATweT1Ljy8zMTHqb2xlI256Sk85jGEu66zQbqUaJUZ3im6k1SikE5s2bF/t51apVvPvuu8Avf+FHIpHY7wzDwOVyAdzUHolEcDqd4+7f5/Ph8/lirwcHB5Man8fjSXqbmWiyj2G21GkyqUaJUZ3im8oaFRUVJdw3pVNEo9Fo7Odjx45RXFwMgNfrpaOjg+vXrxMOh+nv72fx4sUsWrSI/v5+wuEwIyMjdHR04PV6U3lrERFJo7gzgT179tDT08OlS5fYsmUL69ev59tvv+XcuXPYbDby8/PZvHkzAMXFxdx///3U1dVht9t54YUXsNt/yZnnn3+enTt3Mjo6ykMPPRQLDhERmT42c7yF/Bmkr68vqf7pnnbdeOmJ+J0mQcZH/5jU/WsKH59qlBjVKb5ZtRwkIiKzg0JARMTCFAIiIhamEBARsTCFgIiIhSkEREQsTCEgImJhCgEREQtTCIiIWJhCQETEwhQCIiIWphAQEbEwhYCIiIUpBERELEwhICJiYQoBERELUwiIiFiYQkBExMLiPmO4qamJzs5O8vLyqK+vB+DTTz/lxIkTZGZmUlBQgN/v53e/+x3hcJja2trYo81KS0tjzx8+e/YsjY2NXLt2jcrKSjZt2oTNZpvEQxMRkXjihsDKlSt55JFHaGxsjLWVl5fz9NNPk5GRwWeffUZzczPPPPMMAIWFhezevfuW/Xz00Ue8/PLLlJaW8s4779Dd3U1lZWUaD0VERJIVdzloyZIlOByOm9r++Mc/kpGRAUBZWRmGYdx2H9FolCtXrlBWVobNZmPFihWEQqEJDFtERNIh7kwgntbWVqqrq2Ovw+Ewb7zxBtnZ2Tz11FPce++9GIaB2+2O9XG73XGDQ0REJt+EQuDQoUNkZGSwfPlyAJxOJ01NTdx9992cPXuW3bt3U19fj2maSe03GAwSDAYBCAQCeDyepLbPzMxMepvbGUjbnpKTzmMYS7rrNBupRolRneKbqTVKOQSOHj3KiRMneOutt2Jf8GZlZZGVlQVASUkJBQUF9Pf343a7iUQisW0jkQgul2vcfft8Pnw+X+z14OBgUmPzeDxJbzMTTfYxzJY6TSbVKDGqU3xTWaNfT85JREqniHZ3d/PFF1/wl7/8hblz58baL168yOjoKAADAwP09/dTUFCA0+kkOzubU6dOYZom7e3teL3eVN5aRETSKO5MYM+ePfT09HDp0iW2bNnC+vXraW5uZmRkhB07dgD/fypoT08PBw8eJCMjA7vdzksvvRT7UvnFF1+kqamJa9euUVFRoTODRERmAJuZ7IL9NOjr60uqf7qnXTdeeiJt+0pGxkf/mNT9awofn2qUGNUpvlm1HCQiIrODQkBExMIUAiIiFqYQEBGxMIWAiIiFTfi2EbPJdJ0FJCIyXTQTEBGxMIWAiIiFKQRERCxMISAiYmEKARERC1MIiIhYmEJARMTCFAIiIhami8VmsNtdvDbZt5kWEWvQTEBExMIUAiIiFqYQEBGxsIS+E2hqaqKzs5O8vDzq6+sBGB4epqGhgQsXLpCfn09tbS0OhwPTNNm3bx9dXV3MnTsXv99PSUkJAEePHuXQoUMArF27lpUrV07OUYmISEISmgmsXLmSv/71rze1tbS0sHTpUvbu3cvSpUtpaWkBoKuri/Pnz7N37142b97Mxx9/DPwSGp9//jm7du1i165dfP755wwPD6f5cEREJBkJhcCSJUtwOBw3tYVCIWpqagCoqakhFAoBcPz4cVasWIHNZqOsrIzLly8TjUbp7u6mvLwch8OBw+GgvLyc7u7uNB+OiIgkI+XvBIaGhnA6nQA4nU4uXrwIgGEYeDyeWD+3241hGBiGgdvtjrW7XC4Mw0j17UVEJA3Sfp2AaZq3tNlstjH7jtceDAYJBoMABAKBm0IlEZmZmUlvAzCQ9BbTJ5Xj+2+p1slKVKPEqE7xzdQapRwCeXl5RKNRnE4n0WiU3Nxc4Je//AcHB2P9IpEITqcTl8tFT09PrN0wDJYsWTLmvn0+Hz6fL/b6t/tLhMfjSXqbO006js8KdZoo1SgxqlN8U1mjoqKihPumvBzk9Xppa2sDoK2tjaqqqlh7e3s7pmly6tQpcnJycDqdVFRUcPLkSYaHhxkeHubkyZNUVFSk+vYiIpIGCc0E9uzZQ09PD5cuXWLLli2sX7+eNWvW0NDQQGtrKx6Ph7q6OgAqKyvp7Oxk69atzJkzB7/fD4DD4eDJJ59k+/btAKxbt+6WL5tFRGRq2cyxFvFnmL6+vqT6pzrtupMeNJ+OewdpCh+fapQY1Sm+WbccJCIidz6FgIiIhSkEREQsTCEgImJhCgEREQtTCIiIWJhCQETEwhQCIiIWphAQEbEwhYCIiIUpBERELEwhICJiYWl/qIxMjfFudpeOG8uJiHVoJiAiYmEKARERC1MIiIhYmEJARMTCFAIiIhaW8tlBfX19NDQ0xF6Hw2HWr1/P5cuXOXLkCLm5uQBs2LCBZcuWAdDc3Exrayt2u51NmzbpQfMiItMs5RAoKipi9+7dAIyOjvLyyy/zpz/9iS+//JLHHnuMJ564+RTGH3/8kY6ODt5//32i0Sg7duzggw8+wG7XZEREZLqk5X/gb775hsLCQvLz88ftEwqFqK6uJisri/nz51NYWEhvb2863l5ERFKUlovFvvrqKx544IHY68OHD9Pe3k5JSQnPPfccDocDwzAoLS2N9XG5XBiGMeb+gsEgwWAQgEAggMfjSWo8mZmZSW8DMJD0FjNPMsedap2sRDVKjOoU30yt0YRDYGRkhBMnTvD0008DsHr1atatWwfAgQMH2L9/P36/H9M0E96nz+fD5/PFXg8ODiY1Jo/Hk/Q2s0Uyx23lOiVKNUqM6hTfVNaoqKgo4b4TXg7q6uriD3/4A/PmzQNg3rx52O127HY7q1at4syZMwC43W4ikUhsO8MwcLlcE317ERGZgAmHwH8vBUWj0djPx44do7i4GACv10tHRwfXr18nHA7T39/P4sWLJ/r2IiIyARNaDvrPf/7D119/zebNm2Ntn332GefOncNms5Gfnx/7XXFxMffffz91dXXY7XZeeOEFnRkkIjLNbGYyi/XTpK+vL6n+qa69jXdnzjtJMncR1TpufKpRYlSn+GbqdwKWvJX0bPjPXkQkHbQeIyJiYQoBERELUwiIiFiYQkBExMIUAiIiFqYQEBGxMIWAiIiFKQRERCxMISAiYmEKARERC1MIiIhYmEJARMTCLHkDudlsvJvjJXN3URGxDs0EREQsTCEgImJhCgEREQub8HcCr7zyCnfddRd2u52MjAwCgQDDw8M0NDRw4cIF8vPzqa2txeFwYJom+/bto6uri7lz5+L3+ykpKUnHcYiISArS8sXw3/72N3Jzc2OvW1paWLp0KWvWrKGlpYWWlhaeeeYZurq6OH/+PHv37uX06dN8/PHH7Nq1Kx1DEBGRFEzKclAoFKKmpgaAmpoaQqEQAMePH2fFihXYbDbKysq4fPky0Wh0MoYgIiIJSMtMYOfOnQA8/PDD+Hw+hoaGcDqdADidTi5evAiAYRh4PJ7Ydm63G8MwYn1FRGRqTTgEduzYgcvlYmhoiLfffvu2T7k3TfOWNpvNdktbMBgkGAwCEAgEbgqORGRmZt52m4Gk9jY7jFWPeHUS1ShRqlN8M7VGEw4Bl8sFQF5eHlVVVfT29pKXl0c0GsXpdBKNRmPfF7jdbgYHB2PbRiKRMWcBPp8Pn88Xe/3bbRLh8XiS3ma2G6seqlN8qlFiVKf4prJGt/tj/L9N6DuBq1evcuXKldjPX3/9NQsXLsTr9dLW1gZAW1sbVVVVAHi9Xtrb2zFNk1OnTpGTk6OlIBGRaTShmcDQ0BDvvfceADdu3ODBBx+koqKCRYsW0dDQQGtrKx6Ph7q6OgAqKyvp7Oxk69atzJkzB7/fP/EjEBGRlNnMsRbqZ5i+vr6k+sebdo13f53ZbKx7B2kKH59qlBjVKb5ZuRwkIiJ3NoWAiIiFKQRERCxsVj9PwIpr/yIiydBMQETEwhQCIiIWphAQEbEwhYCIiIUpBERELEwhICJiYQoBERELUwiIiFiYQkBExMJm9RXD8v/Gunp6gLHvLioi1qGZgIiIhSkEREQsTCEgImJhCgEREQtL+YvhwcFBGhsb+emnn7DZbPh8Ph599FEOHjzIkSNHyM3NBWDDhg0sW7YMgObmZlpbW7Hb7WzatImKior0HIWIiKQk5RDIyMjg2WefpaSkhCtXrrBt2zbKy8sBeOyxx3jiiZvPRvnxxx/p6Ojg/fffJxqNsmPHDj744APsdk1GRESmS8oh4HQ6cTqdAGRnZ7NgwQIMwxi3fygUorq6mqysLObPn09hYSG9vb2UlZWlOgRJg/EevKNTR0WsIS1/hofDYb7//nsWL14MwOHDh3nttddoampieHgYAMMwcLvdsW1cLtdtQ0NERCbfhC8Wu3r1KvX19WzcuJGcnBxWr17NunXrADhw4AD79+/H7/djmmbC+wwGgwSDQQACgQAejyepMWVmZuLxeBhIaiv5rWRrPlv9+lmS21Od4pupNZpQCIyMjFBfX8/y5cu57777AJg3b17s96tWreLdd98FwO12E4lEYr8zDAOXyzXmfn0+Hz6fL/Z6cHAwqXF5PJ6kt5GbqX6/0GcpMapTfFNZo6KiooT7prwcZJomH374IQsWLODxxx+PtUej0djPx44do7i4GACv10tHRwfXr18nHA7T398fWz4SEZHpkfJM4LvvvqO9vZ2FCxfy+uuvA7+cDvrVV19x7tw5bDYb+fn5bN68GYDi4mLuv/9+6urqsNvtvPDCCzozSERkmtnMZBbrp0lfX19S/X+ddo135ovEp7ODfqFljsSoTvHN1OUg3UVUxqRTR0WsQesxIiIWphAQEbEwhYCIiIUpBERELEwhICJiYQoBEREL0ymikhSdOioyu2gmICJiYQoBERELUwiIiFiYQkBExMIUAiIiFqYQEBGxMIWAiIiF6ToBSQtdPyByZ1IIyKRSOIjMbFoOEhGxsCmfCXR3d7Nv3z5GR0dZtWoVa9asmeohyAyQ7KM/NXMQmRxTGgKjo6N88sknvPnmm7jdbrZv347X6+Wee+6ZymHIHeh2oaGAEEndlIZAb28vhYWFFBQUAFBdXU0oFFIIyIRoViGSuikNAcMwcLvdsddut5vTp09P5RBEkgqNgTS+r8JHZqIpDQHTNG9ps9lst7QFg0GCwSAAgUCAoqKipN+rqKgI/vd48oMUkZSk8u/UamZijab07CC3200kEom9jkQiOJ3OW/r5fD4CgQCBQCCl99m2bVvKY7QS1Sk+1SgxqlN8M7VGUxoCixYtor+/n3A4zMjICB0dHXi93qkcgoiI/MaULgdlZGTw/PPPs3PnTkZHR3nooYcoLi6eyiGIiMhvTPl1AsuWLWPZsmWT+h4+n29S9z9bqE7xqUaJUZ3im6k1spljfVsrIiKWoNtGiIhY2Ky7gZxuSzG2V155hbvuugu73U5GRgaBQIDh4WEaGhq4cOEC+fn51NbW4nA4pnuoU6qpqYnOzk7y8vKor68HGLcupmmyb98+urq6mDt3Ln6/n5KSkmk+gqkxVp0OHjzIkSNHyM3NBWDDhg2xpd7m5mZaW1ux2+1s2rSJioqKaRv7VBkcHKSxsZGffvoJm82Gz+fj0UcfnfmfJ3MWuXHjhvnqq6+a58+fN69fv26+9tpr5g8//DDdw5oR/H6/OTQ0dFPbp59+ajY3N5umaZrNzc3mp59+Oh1Dm1bffvuteebMGbOuri7WNl5dTpw4Ye7cudMcHR01v/vuO3P79u3TMubpMFadDhw4YH7xxRe39P3hhx/M1157zbx27Zo5MDBgvvrqq+aNGzemcrjTwjAM88yZM6ZpmubPP/9sbt261fzhhx9m/OdpVi0H/fa2FJmZmbHbUsjYQqEQNTU1ANTU1FiyVkuWLLll9jNeXY4fP86KFSuw2WyUlZVx+fJlotHolI95OoxVp/GEQiGqq6vJyspi/vz5FBYW0tvbO8kjnH5OpzP2l3x2djYLFizAMIwZ/3maVctBui3F7e3cuROAhx9+GJ/Px9DQUOxiPafTycWLF6dzeDPGeHUxDAOPxxPr53a7MQxjzAsereLw4cO0t7dTUlLCc889h8PhwDAMSktLY31cLheGYUzjKKdeOBzm+++/Z/HixTP+8zSrQsBM8LYUVrRjxw5cLhdDQ0O8/fbbM/Ly9ZlOn6+brV69mnXr1gFw4MAB9u/fj9/vH7NOVnL16lXq6+vZuHEjOTk54/abKZ+nWbUclOhtKazI5XIBkJeXR1VVFb29veTl5cWmn9FoNPYFn9WNVxe3283g4GCsn9U/X/PmzcNut2O321m1ahVnzpwBbv13aBhG7PM3242MjFBfX8/y5cu57777gJn/eZpVIaDbUozt6tWrXLlyJfbz119/zcKFC/F6vbS1tQHQ1tZGVVXVdA5zxhivLl6vl/b2dkzT5NSpU+Tk5Fg6BH67fn3s2LHY1f9er5eOjg6uX79OOBymv7+fxYsXT9cwp4xpmnz44YcsWLCAxx9/PNY+0z9Ps+5isc7OTv7+97/Hbkuxdu3a6R7StBsYGOC9994D4MaNGzz44IOsXbuWS5cu0dDQwODgIB6Ph7q6OsudIrpnzx56enq4dOkSeXl5rF+/nqqqqjHrYpomn3zyCSdPnmTOnDn4/X4WLVo03YcwJcaq07fffsu5c+ew2Wzk5+ezefPm2H9ihw4d4ssvv8Rut7Nx40YqKyun+Qgm37///W/eeustFi5cGFvW2bBhA6WlpTP68zTrQkBERBI3q5aDREQkOQoBERELUwiIiFiYQkBExMIUAiIiFqYQEBGxMIWAiIiFKQRERCzs/wCtKAsKXEfvDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, GRU, ConvLSTM2D\n",
    "from keras.layers import TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "max_len = 150\n",
    "lstm_n = 256\n",
    "batch_n = 25\n",
    "epoch_n = 4\n",
    "\n",
    "word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}\n",
    "word2idx['_PAD_'] = 0\n",
    "word2idx['_OOV_'] = 1\n",
    "\n",
    "tag2idx = {'O': 0, 'B': 1, 'I': 2}\n",
    "n_tags = len(tag2idx)\n",
    "\n",
    "X = [[word2idx.get(m, 1) for m in row[1]] for row in training_data]\n",
    "y = [[tag2idx.get(m, 0) for m in row[2]] for row in training_data]\n",
    "\n",
    "# Get an idea about class dist.\n",
    "cnt = Counter()\n",
    "for labels in y:\n",
    "    cnt.update(labels)\n",
    "cnt_total = sum([v for _, v in cnt.items()])\n",
    "for k, v in cnt.items():\n",
    "    print(\"{0}: {1}\".format(k, (100 * v)/cnt_total))\n",
    "        \n",
    "\n",
    "# Get a general idea of the lengths of the sequences:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.hist([len(s) for s in X], bins=50)\n",
    "plt.show()\n",
    "\n",
    "X_test = [[word2idx.get(m, 1) for m in row[1]] for row in test_data]\n",
    "y_test = [[tag2idx.get(m, 1) for m in row[2]] for row in test_data]\n",
    "\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['_PAD_'])\n",
    "X_test = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\", value=word2idx['_PAD_'])\n",
    " \n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx['O']) \n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag2idx['O']) \n",
    "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Embeddings (GloVe)\n",
    "\n",
    "See: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "The embeddings loaded below were learned from PMC Open Access dataset (non-commercial). This collection contains 430,944 journal articles in XML (or text) format. The XML alone is over 50GB uncompressed, so only download if you have plenty of drive space and time. Tables were filtered out since they tended to contain numeric data and not narrative text. The final cleaned text is 1,784,578,876 tokens, and 958,634 lines of text!\n",
    "\n",
    "This first attempt at using pre-trained embeddings from biomedical journal articles didn't improve the accuracy over learning embeddings directly from the training data, but it is useful to demonstrate how these can be added to the model. Run the code below to load the pre-trained PMC embeddings. You will need to add `weights=[embedding_matrix]` and `trainable=False` as arguments to the model's Embedding layer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embeddings_dir = r'/home/ryan/Development/deep-learn-bio-nlp/'\n",
    "embedding_dim = 256\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(embeddings_dir, 'vectors_100K.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2idx) + 1, embedding_dim))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "embedding_dim = 100\n",
    "\n",
    "fasttext_model = fasttext.load_model('./ftmodel.bin')\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2idx) + 1, embedding_dim))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = fasttext_model[word].tolist()\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "This is the first version of the model using a single Bidirectional LSTM layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras_contrib-2.0.8-py3.7.egg/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "/home/ryan/anaconda3/envs/mlenv/lib/python3.7/site-packages/keras_contrib-2.0.8-py3.7.egg/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 150, 100)          3665800   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 150, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 150, 512)          548352    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 150, 3)            1539      \n",
      "_________________________________________________________________\n",
      "crf_2 (CRF)                  (None, 150, 3)            27        \n",
      "=================================================================\n",
      "Total params: 4,215,718\n",
      "Trainable params: 4,215,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Borrowed heavily from \n",
    "# https://appliedmachinelearning.blog/2019/04/01/training-deep-learning-based-named-entity-recognition-from-scratch-disease-extraction-hackathon/\n",
    "# as a general outline for the model...\n",
    "\n",
    "from keras.layers.merge import add\n",
    "\n",
    "_input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=len(word2idx) + 1, \n",
    "                  output_dim=embedding_dim, \n",
    "                  # weights=[embedding_matrix], \n",
    "                  input_length=max_len, \n",
    "                  # trainable=False, \n",
    "                  mask_zero=True)(_input)\n",
    "model = Dropout(0.2)(model)\n",
    "model = Bidirectional(GRU(units=lstm_n, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
    "model = Model(_input, out)\n",
    " \n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14250 samples, validate on 750 samples\n",
      "Epoch 1/4\n",
      "14250/14250 [==============================] - 152s 11ms/step - loss: 8.6891 - crf_viterbi_accuracy: 0.8986 - val_loss: 7.2945 - val_crf_viterbi_accuracy: 0.9373\n",
      "Epoch 2/4\n",
      "14250/14250 [==============================] - 150s 11ms/step - loss: 8.6090 - crf_viterbi_accuracy: 0.9013 - val_loss: 7.2762 - val_crf_viterbi_accuracy: 0.9373\n",
      "Epoch 3/4\n",
      "14250/14250 [==============================] - 149s 10ms/step - loss: 8.5913 - crf_viterbi_accuracy: 0.9015 - val_loss: 7.2670 - val_crf_viterbi_accuracy: 0.9373\n",
      "Epoch 4/4\n",
      "14250/14250 [==============================] - 150s 11ms/step - loss: 8.5823 - crf_viterbi_accuracy: 0.9015 - val_loss: 7.2624 - val_crf_viterbi_accuracy: 0.9373\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, np.array(y), batch_size=batch_n, epochs=epoch_n, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and review the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r+-', label='Validation accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r+-', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = model.predict(X_test)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 150)\n"
     ]
    }
   ],
   "source": [
    "pred_index = np.argmax(pred, axis=-1)\n",
    "print(pred_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code converts the output of the tagger (where each term is BIO tagged) back to the original BCII format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open('{0}/ryan_eval.eval'.format(test_home), 'w') as mention_fh:\n",
    "    for i, row in enumerate(test_data):\n",
    "        sent_id = row[0]\n",
    "        tokens = row[1][:max_len]\n",
    "        running_count = 0\n",
    "        buffer = {'s':None, 'e':None, 'text':[]}\n",
    "        for j, token in enumerate(tokens):\n",
    "            tag = pred_index[i][j]\n",
    "            if tag == 1:\n",
    "                buffer.get('text').append(token)\n",
    "                buffer['s'] = running_count\n",
    "                buffer['e'] = running_count + len(token) - 1\n",
    "            elif tag == 2:\n",
    "                buffer.get('text').append(token)\n",
    "                buffer['e'] = running_count + len(token) - 1\n",
    "            elif tag == 0 and buffer.get('s'):\n",
    "                text = ' '.join(buffer.get('text'))\n",
    "                mention_fh.write('{0}|{1} {2}|{3}\\n'.format(sent_id, buffer.get('s'), buffer.get('e'), text))\n",
    "                buffer = {'s':None, 'e':None, 'text':[]}\n",
    "            running_count += len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BCII evaluation script, initial output on the test data is mixed (note that your output may vary). The precision is in the range of the shared task participants, but the recall leaves something to be desired -- especially the high number of false negatives (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2559986/). \n",
    "\n",
    "    TP: 4142\n",
    "    FP: 1008\n",
    "    FN: 2189\n",
    "    Precision: 0.804271844660194 Recall: 0.654241036171221 F: 0.721539935545684\n",
    "\n",
    "This F score isn't bad, but is also isn't great. \n",
    "This score does place this system above the bottom 3 of 19 participants in the 2008 competition. Note that _only_ the training data was used (what the task describes as a _closed_ system), not using any outside resources. This is a good starting point.\n",
    "\n",
    "One possible issue is that bio-medical journal articles have a very rich\n",
    "vocabulary. This has the following effects:\n",
    "\n",
    "* Any unseen text is likely to have a high number of OOV (unknown) words. \n",
    "* Using standard embeddings which look at complete words as input, we lose out\n",
    "  on words that may have similar morphology.\n",
    "  \n",
    "For example, in the case of gene names, we may be missing common suffixes or \n",
    "patterns in names and/or symbols.\n",
    "\n",
    "## Investigating sub-word embeddings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
