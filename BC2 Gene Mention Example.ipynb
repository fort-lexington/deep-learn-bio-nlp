{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioCreative II Gene Mention (GM) Task\n",
    "\n",
    "For more information: https://biocreative.bioinformatics.udel.edu/tasks/biocreative-ii/task-1a-gene-mention-tagging/\n",
    "\n",
    "## Training Data\n",
    "\n",
    "The training data is described in the corpus README.GM file, but I'll describe it here as well. Training data consists of a sentences file `train.in` and a label file `GENE.eval` while lists the offsets of any gene mentions (there may be none for any sentence). It is easiest to understand using an example using the first sentence from `train.in`:\n",
    "\n",
    "```\n",
    "P00001606T0076 Comparison with alkaline phosphatases and 5-nucleotidase\n",
    "```\n",
    "Each line contains a single sentence, starting with a unique sentence identifier, followed by the text. This particular sample contains two (2) gene mentions, listed on two lines in the `GENE.eval` file:\n",
    "\n",
    "```\n",
    "P00001606T0076|14 33|alkaline phosphatases\n",
    "P00001606T0076|37 50|5-nucleotidase\n",
    "```\n",
    "The first field (delimied by the bar symbols) is the matching sentence ID. The second field contains the offset of the first and last characters in the GM, *not counting space characters*. So, looking at *alkaline phosphatases*, the first letter *a* is at offset 14 keeping in mind that the first character in the sentence is offset 0. If you are not careful, you may think the offset of *a* is 16, but remember that spaces are not counted. Counting in a similar way, the last *s* in *phosphatases* is at offset 33.\n",
    "\n",
    "## Prepare the training data\n",
    "\n",
    "The format is not very convenient for training our ML model. One method used to train NER systems to label each sentence token with either 'B','I', or 'O' where 'B' marks the beginning token in an entity, 'I' marks subsequent tokens in a multi-token entity (*inside*), and 'O' is for tokens *outside* the entity.\n",
    "\n",
    "The module *bc2reader.py* will help convert these two files to something more usable. The first argument to the `BC2Reader` contructor is the sentence file. The second is the gene mention file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bc2reader import BC2Reader\n",
    "\n",
    "train_home = '/home/ryan/Development/deep-learn-bio-nlp/bc2/bc2geneMention/train'\n",
    "reader = BC2Reader('{0}/train.in'.format(train_home), '{0}/GENE.eval'.format(train_home))\n",
    "reader.convert('{0}/train.json'.format(train_home))\n",
    "vocab = list(reader.vocab) # it's a set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a JSON file with a more familiar format. Here is the first sentence in our BIO format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P00001606T0076', ['Comparison', 'with', 'alkaline', 'phosphatases', 'and', '5-nucleotidase'], ['O', 'O', 'B', 'I', 'O', 'B']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('{0}/train.json'.format(train_home), 'r') as json_file:\n",
    "    training_data = json.load(json_file)\n",
    "    print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be easier to read if we zip together the tokens and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Comparison', 'O'), ('with', 'O'), ('alkaline', 'B'), ('phosphatases', 'I'), ('and', 'O'), ('5-nucleotidase', 'B')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(training_data[0][1], training_data[0][2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BC2GM000008491', ['Phenotypic', 'analysis', 'demonstrates', 'that', 'trio', 'and', 'Abl', 'cooperate', 'in', 'regulating', 'axon', 'outgrowth', 'in', 'the', 'embryonic', 'central', 'nervous', 'system', '(', 'CNS', ')', '.'], ['O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "test_home = '/home/ryan/Development/deep-learn-bio-nlp/bc2/bc2GNandGMgold_Subs/sourceforgeDistrib-22-Sept-07/genemention/BC2GM/test'\n",
    "reader = BC2Reader('{0}/test.in'.format(test_home), '{0}/GENE.eval'.format(test_home))\n",
    "reader.convert('{0}/test.json'.format(test_home))\n",
    "with open('{0}/test.json'.format(test_home), 'r') as json_file:\n",
    "    test_data = json.load(json_file)\n",
    "    print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.layers import TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "max_len = 200\n",
    "lstm_n = 128\n",
    "batch_n = 48\n",
    "epoch_n = 3\n",
    "\n",
    "word2idx = {word: idx + 2 for idx, word in enumerate(vocab)}\n",
    "word2idx['_PAD_'] = 0\n",
    "word2idx['_OOV_'] = 1\n",
    "tag2idx = {'O': 0, 'B': 1, 'I': 2}\n",
    "n_tags = 3\n",
    "\n",
    "X = [[word2idx.get(m, 1) for m in row[1]] for row in training_data]\n",
    "y = [[tag2idx.get(m, 1) for m in row[2]] for row in training_data]\n",
    "\n",
    "X_test = [[word2idx.get(m, 1) for m in row[1]] for row in test_data]\n",
    "y_test = [[tag2idx.get(m, 1) for m in row[2]] for row in test_data]\n",
    "\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['_PAD_'])\n",
    "X_test = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\", value=word2idx['_PAD_'])\n",
    " \n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"]) \n",
    "y = [to_categorical(i, num_classes=3) for i in y]\n",
    "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag2idx[\"O\"]) \n",
    "y_test = [to_categorical(i, num_classes=3) for i in y_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13500 samples, validate on 1500 samples\n",
      "Epoch 1/3\n",
      "13500/13500 [==============================] - 83s 6ms/step - loss: 0.0644 - acc: 0.9860 - val_loss: 0.0311 - val_acc: 0.9890\n",
      "Epoch 2/3\n",
      "13500/13500 [==============================] - 80s 6ms/step - loss: 0.0230 - acc: 0.9914 - val_loss: 0.0212 - val_acc: 0.9924\n",
      "Epoch 3/3\n",
      "13500/13500 [==============================] - 80s 6ms/step - loss: 0.0125 - acc: 0.9954 - val_loss: 0.0205 - val_acc: 0.9930\n"
     ]
    }
   ],
   "source": [
    "# Borrowed heavily from \n",
    "# https://appliedmachinelearning.blog/2019/04/01/training-deep-learning-based-named-entity-recognition-from-scratch-disease-extraction-hackathon/\n",
    "# as a general outline for the model...\n",
    "\n",
    "input = Input(shape=(max_len,))\n",
    " \n",
    "model = Embedding(input_dim=len(vocab), output_dim=lstm_n, input_length=max_len)(input)\n",
    "model = Dropout(0.2)(model)\n",
    "model = Bidirectional(LSTM(units=lstm_n, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
    "model = Model(input, out)\n",
    " \n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, np.array(y), batch_size=batch_n, epochs=epoch_n, validation_split=0.10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 200)\n"
     ]
    }
   ],
   "source": [
    "pred_index = np.argmax(pred, axis=-1)\n",
    "print(pred_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to convert back to the original BCII format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open('{0}/ryan_eval.eval'.format(test_home), 'w') as mention_fh:\n",
    "    for i, row in enumerate(test_data):\n",
    "        sent_id = row[0]\n",
    "        tokens = row[1][:max_len]\n",
    "        running_count = 0\n",
    "        buffer = {'s':None, 'e':None, 'text':[]}\n",
    "        for j, token in enumerate(tokens):\n",
    "            tag = pred_index[i][j]\n",
    "            if tag == 1:\n",
    "                buffer.get('text').append(token)\n",
    "                buffer['s'] = running_count\n",
    "                buffer['e'] = running_count + len(token) - 1\n",
    "            elif tag == 2:\n",
    "                buffer.get('text').append(token)\n",
    "                buffer['e'] = running_count + len(token) - 1\n",
    "            elif tag == 0 and buffer.get('s'):\n",
    "                text = ' '.join(buffer.get('text'))\n",
    "                mention_fh.write('{0}|{1} {2}|{3}\\n'.format(sent_id, buffer.get('s'), buffer.get('e'), text))\n",
    "                buffer = {'s':None, 'e':None, 'text':[]}\n",
    "            running_count += len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BCII evaluation script, initial output on the test data is mixed. The precision is actually in linewith the shared task participants, but the recall leaves something to be desired (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2559986/):\n",
    "    \n",
    "    TP: 3122\n",
    "    FP: 668\n",
    "    FN: 3209\n",
    "    Precision: 0.823746701846966 Recall: 0.493129047543832 F: 0.616935085465863\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
