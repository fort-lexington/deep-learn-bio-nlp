{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For a background corpus to build the GloVe embeddings, I used the complete PubMed CentralÂ® (PMC) Open Access non-commercial subset. The files used were downloaded on July 30, 2019. See the [PMC FTP](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/) and [PMC About](https://www.ncbi.nlm.nih.gov/pmc/about/intro/) for information on accessing the data using FTP. \n",
    "\n",
    "(GloVe: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.)\n",
    "\n",
    "Here are some statistics about the downloaded articles:\n",
    "\n",
    "* 958,634 articles (one .nxml file per article).\n",
    "* XML files compressed are about 11.1 GB, 51.5 GB uncompressed (!)\n",
    "* I'm using XML in order to exclude tables (where collocation data isn't very meaningful, especially when the tables are numeric).\n",
    "\n",
    "# Preprocessing the Corpus\n",
    "\n",
    "I have included a simple python module `corpus.py` that simplifies the process of iterating over the entire corpus. At each step, a list of tokens from the next article is returned (punctuation removed). This module is included in this repository.\n",
    "\n",
    "Each article's token list is written as a space delimited string into the file `corpus.txt`. Documents are separated by a newline character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import DocumentCorpus\n",
    "\n",
    "PMC = r'/media/ryan/ExtraDrive1/PMC/XML/'\n",
    "\n",
    "docs = DocumentCorpus(root = PMC)\n",
    "with open('corpus.txt', 'w') as output:\n",
    "    for doc in docs:\n",
    "        text = ' '.join(doc)\n",
    "        output.write(\"\\n\")  # end of article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the GloVe model\n",
    "\n",
    "The first step is to download and build GloVe from [StanfordNLP GitHub](https://github.com/stanfordnlp/GloVe). Note that I was only able to run the python evaluation script using python 2.7 (and numpy).\n",
    "\n",
    "Here is a simple shell script to take the text corpus generated in the previous step, and output a GloVe model. I am mostly using GloVe defaults:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "MODEL_DIR=/home/ryan/Development/deep-learn-bio-nlp\n",
    "GLOVE_DIR=/home/ryan/Development/GloVe-1.2/build\n",
    "MAX_VOCAB=100000\n",
    "MODEL_NAME=vectors_100K\n",
    "GLOVE_ITER=7\n",
    "OUTPUT_DIM=100\n",
    "# Example using mostly GloVe defaults, with a few exceptions\n",
    "\n",
    "$GLOVE_DIR/vocab_count -max-vocab $MAX_VOCAB -min-count 10 < $MODEL_DIR/corpus.txt > $MODEL_DIR/vocab.txt\n",
    "\n",
    "$GLOVE_DIR/cooccur -window-size 10 -vocab-file $MODEL_DIR/vocab.txt < $MODEL_DIR/corpus.txt > $MODEL_DIR/cooccurrences.bin\n",
    "\n",
    "$GLOVE_DIR/shuffle -verbose 0  < $MODEL_DIR/cooccurrences.bin > $MODEL_DIR/cooccurrences.shuf.bin\n",
    "\n",
    "$GLOVE_DIR/glove -iter $GLOVE_ITER -binary 2 -vector-size 256 -input-file $MODEL_DIR/cooccurrences.shuf.bin -vocab-file $MODEL_DIR/vocab.txt -save-file $MODEL_NAME\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
